\chapter{Sequential}
\label{cap:sequential}
In this Chapter we define the structure and the implementation detail of the sequential version of our code. This baseline is usefull to study the performance differences with the parallel version described in Chapter~\ref{cap:parallel}; this analaysis will be in chapter~\ref{cap:analysis}.

Thi version of code is in \href{https://github.com/edoardosarri24/time-series-pattern-recognition/blob/730e3003e125bb1180bfccfef96649d08320eafd/sequential}{sequential} folder within the main repository.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pipeline}
Supposing that the file already is in \texttt{data/input.txt}, out sequential version follow the following steps:
\begin{enumerate}
    \item Data Loading \\
        We allocate memory and load in it the input file. We reorganized the data into a Array of Structures (AoS) memory pattern. Implementation is in \href{https://github.com/edoardosarri24/time-series-pattern-recognition/blob/sequential_documentation/sequential/src/data_loader.cpp}{data\_loader.cpp} file.
    \item Query Loading \\
        We load the query sequence of length $M$ from the file \textit{data/query.txt}. As described in Section~\ref{sec:input}, the noise generation and extraction are handled by the external data preparation script. The implementation is in \texttt{query\_loader.cpp} file.
    \item Pattern Matching \\
        We use a sliding window to scan the input, calculating a SAD of lenght $M$ for each istant time. The implementation is in \href{https://github.com/edoardosarri24/time-series-pattern-recognition/blob/730e3003e125bb1180bfccfef96649d08320eafd/sequential/src/SAD_distance.cpp}{SAD\_distance.cpp} file.
    \item Reporting \\
        We return the index of the best match and the relative SAD value. The ground truth index is displayed by the data generation script for manual verification.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Loading}
\label{sec:data_loading}
Given the large size of the input dataset ($\approx 3.5$ GB in our experiments), standard C++ streams (\texttt{std::ifstream}) introduce significant overhead due to the double coping (in the kernel memory and after in the user's buffer) and repeated system calls. To address this bottleneck, we adopt a Memory Mapped I/O (\texttt{mmap}) strategy: the file is mapped directly into the process virtual address space, allowing the application to access the file content as a contiguous byte array.

In this way we increase the efficincy, but we lost the semplicity of stream parsing and we have to do all stuff by hand. To gain the optimization we must return to the pointer apporch of C.

Additionally, since the parsing logic accesses memory strictly linearly, we utilize the \texttt{madvise} system call with the \texttt{MADV\_SEQUENTIAL} flag. In this way we instructs the kernel to optimized memory usage and prefetch the subsequent pages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Memory Layout}
\label{sec:seq_memory}
For the sequential implementation, we adopt the Array of Structures (AoS) layout. In this configuration, all $D$ dimensions of a single timestamp are stored contiguously in memory: the dataset is represented as a flat vector like $[x[1,1],\cdots,x[1,D],x[2,1], \\ \cdots,x[2,D],\cdots,x[N,1],\cdots,x[N,D]]$. If we want access to the $d$-th dimension of $t$-th timestamp element we can use $f(t,d) = buffer[t \cdot D + d]$.

Since we adopt a sliding window approach, this memory patter technique allows us to maximize the spatial location during the distance calculation. The algorithm, described in Section~\ref{sec:seq_distance}, infact have a inner loop through the $D$ dimension, and a outside loop through the input lenght $N$; when we load in cache the $d$-th dimension of the $t$-th timestamp, we laod also the near values in the same cache line, and these values are processed in the next iterations.

\subsection{Vectorization}
This structure of the data allows us to implement an auto-vectorization without writing assembly code. With the correct compiling flags (i.e., \texttt{-march=native} and \texttt{-O3}), the compiler generates specific instructions for the host CPU.

However we have to give a hint to the compiler that we are sure that vectorializzation is possible. In particoular we have to declar \texttt{restrict} clause in \href{https://github.com/edoardosarri24/time-series-pattern-recognition/blob/f43737e7a58e0ead58fde8862c04cf7b19551603/sequential/src/SAD_distance.cpp}{SAD\_distance.cpp} file to unlock full SIMD operations: in this way we inform the compiler that the \texttt{data} and \texttt{query} arrays never overlap.

\subsection{Padding}
\label{sec:seq_padding}

Our data are a collection of float numbers and for each timestamp we have $D=6$ dimension; we can see the data like a $N\times D$ matrix, where $N$ is the number of elements. The cache lines are composed tipically by 64 Byte and the CPU works better if the data are alligned with this value. Since a float is load in 4 Byte, 6 floats occupies 24 Byte; we can optimize the performance introducing two more dimension (i.e., two more floats) in the way one time stamp occupies 32 Byte in the cache line.

This padding approach doesn't change the SAD distance result, but allow us to have low cache miss and also optimize the vetorization described above. On the other hand we have a waste of memory of 25\% due to padding. We implemented an automatic padding mechanism that adjusts the inner dimension of the dataset to be a divisor of the cache line size (typically 64 Bytes). This alignment ensures that data loads are efficient and fully utilize the memory bandwidth.

\myskip

The above concept is correct in teory, but the real test say us a differnt thinks. The Section~\ref{sec:analysis_padding} show these results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{SAD distance}
\label{sec:seq_distance}
The SAD metric calculation is performed using a Brute Force Sliding Window approach. If $Q$ be the query of length $M$, we search for the starting index $i^*$ that minimizes the distance: $\displaystyle i^*=\arg\min_{i\in[0,N-M]}D(i)$, where = $\displaystyle D(i)=\sum_{m=0}^{M} \sum_{d=0}^{D} |Q[m,d] - T[i+m,d]|$.

\subsection{Early Abandoning}
\label{sec:seq_earling_abandoning}
To optimize the sequential execution, we usually implement an early abandoning approach: during the computation of $D(i)$, we maintain the current distance; if the partial local sum exceeds the current global minimum distance found ($min\_dist$), the computation for the current window $i$ is immediately halted.

However, this technique creates a significant algorithmic disparity when comparing with a parallel GPU implementation, but instead we want compare "apple with apple". On the GPU, thread divergence makes early abandoning less effective or counter-productive for this specific kernel structure. So we have parametrized it with the \texttt{ENABLE\_EARLY\_ABANDONING} compilation flag, that is \texttt{false} by default.

To see this analysis go to Section~\ref{sec:analysis_early_abandoning}.