\chapter{Introduction}
This project is about the C++ implementation of a program that, given a pattern (i.e., a query), finds the closest subsequence in a time series input. There are two implementations: the first is a sequential approach, described in Chapter~\ref{cap:sequential}; the second uses the CUDA framework, described in Chapter~\ref{cap:parallel}, to obtain the maximum speed-up.

This is the \href{https://github.com/edoardosarri24/time-series-pattern-recognition}{repository} of the project.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Approach}
\label{sec:distance}
To find the best match the approach used is the sliding window: for each time instant we compute a distance between the query and the input. After this we return the best part of the input that is closest to the query. If $Q$ is the length of the query and $I$ is the length of the input, we have $I\gg Q$).

The distance between the query and the input window is the Sum of Absolute Differences (SAD). Let $Q$ be the query of length $M$; we search for the starting index $i^*$ that minimizes the distance: $\displaystyle i^*=\arg\min_{i\in[0,N-M]}D(i)$, where $\displaystyle D(i)=\sum_{m=0}^{M} \sum_{d=0}^{D} |Q[m,d] - T[i+m,d]|$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset}
\label{sec:dataset}
We selected the \href{https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones}{Human Activity Recognition} (HAR) dataset. This is a multivariate time-series dataset with $D=6$ dimensions per timestamp: 3-axial linear acceleration (\textit{acc\_X}, \textit{acc\_Y}, \textit{acc\_Z}) and 3-axial angular velocity (\textit{gyro\_X}, \textit{gyro\_Y}, \textit{gyro\_Z}).

The dataset consists of 10299 samples (7352 for training and 2947 for testing). Each sample captures 2.56 seconds of activity recorded at 50Hz, resulting in a window length of 128 timestamps. In this scenario, we concatenate the dataset samples to generate a continuous stream of multi-dimensional data points, resulting in a total of circa 7.9 million floating-point values to process.

The result is that the dataset is downloaded to the \texttt{data/input.txt} file, where all samples are concatenated. Each element of a sample is represented by a line with 6 float values (the dimensions of HAR), which are respectively \textit{acc\_X}, \textit{acc\_Y}, \textit{acc\_Z}, \textit{gyro\_X}, \textit{gyro\_Y}, \textit{gyro\_Z}.

\subsection{Multiplier}
\label{sec:multiplier}
To address the limited size of the original dataset the system includes a data augmentation mechanism controlled by a \textit{multiplier M}.

The mechanism is implemented as a pre-processing step during data download: the system iterates through the complete set of original samples \texttt{M} times; the first iteration preserves the original raw values; for every subsequent iteration required to reach the target multiplication factor, the system generates a new variation of the entire dataset, by injecting uniform random noise $\epsilon$ (by default $\epsilon\pm0.01$) into the signal values.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Input and query}
\label{sec:input}
To define the input and the query we have to address two aspects:
\begin{itemize}
    \item Input \\
        We concatenate all samples of the dataset, both the training and test partitions. When the download script has finished the \textit{input.txt} file has the following format: each value of the same timestamp is separated by a single space; two timestamps are separated by the $\backslash$\textit{n} character.

        We perform a data-augmentation process to increment the number of samples because our dataset is too small. We can define the number of copies (1 by default) and the added noise ($\pm$0.01 by default). The new sample is defined as $new=old+\epsilon$, where $\epsilon\in[-0.01,0.01]$.
    \item Query \\
        We sample a random index of the input and extract a query of length $M$. The default choice for the length of the action is $M=64$, but it's also possible to specify it at compilation time. The data preparation script extracts this query, adds Gaussian noise ($\mu=0, \sigma=0.01$) to ensure $SAD > 0$, and saves it to \texttt{data/query.txt}. The original start index is reported by the script for verification.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tools}
To validate and test our software we use two main tools: the sanitizers and the profiler.

\subsection{Sanitizers}
A sanitizer is a dynamic code analyzer integrated in the compiler that allows us to discover runtime errors in our code. The tool instruments the code to store metadata for each variable and this allows detecting runtime errors that the compiler can't see because it works in a static way.

The sanitizer is useful during the development phase. In the release version we compile without the sanitizer because it introduces complexity and leads to performance degradation.

\myskip

The sanitizer tools that we used are the following:
\begin{itemize}
    \item Address (ASan) \\
        It is useful for the detection of memory corruption errors like buffer overflow, use-after-free and memory leaks.
    \item Undefined (UBSan) \\
        It is useful for the detection of undefined behavior in the code, i.e., portions of code that are not conformant to the standard.
    \item Memory (MSan) \\
        Detects the use of uninitialized memory. On macOS it is not supported, but it is present anyway.
\end{itemize}

\subsection{Profiler}
\label{sec:profiler}
Profiling is a technique for analyzing the performance of our software: it is useful to understand where the code spends most of its time and how many times functions are called.

There are two techniques that profilers use: sampling, which is more efficient but less precise; and instrumentation, which is very accurate but more expensive.

\myskip

In our project we used \href{https://developer.ridgerun.com/wiki/index.php/Profiling_with_GPerfTools}{GPerfTools}. We install it with HomeBrew, but this doesn't install the \textit{pprof} tool for the analysis. The new version, found \href{https://github.com/google/pprof}{here}, is developed by Google in Go.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Execution}
The execution of the project requires two steps:
\begin{enumerate}
    \item To download the input we have to execute the \texttt{exec/download\_input.sh} script. This Python script orchestrates the dataset preparation:
    \begin{itemize}
        \item Downloads the original UCI HAR Dataset (ZIP format) from the official repository.
        \item Extracts the raw inertial signals from the archive.
        \item Processes the data by concatenating the training and test sets.
        \item Applies data augmentation, as described in Section~\ref{sec:multiplier}, to generate the final \texttt{data/input.txt} file used by the C++ application.
        \item Extracts a random query sequence, applies noise, and saves it to \texttt{data/query.txt}, printing the ground truth index to standard output.
    \end{itemize}
    \item Finally we can execute one of the scripts in \texttt{exec/} folder. The choice of script depends on the specific requirement: see the \href{https://github.com/edoardosarri24/time-series-pattern-recognition/blob/fff8378275881160eefdc64ad2dfffec7894f612/README.md}{README} for more details.
\end{enumerate}