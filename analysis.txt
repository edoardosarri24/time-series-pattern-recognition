# Suggested Additional Experiments for Report Analysis

Based on the analysis of your current codebase and report, here are several additional experiments you can perform to enrich your performance analysis.

## 1. CUDA Block Size Analysis (Occupancy)
*   **Goal:** Find the optimal number of threads per block to maximize GPU occupancy.
*   **Experiment:** Vary the block size (e.g., 32, 64, 128, 256, 512, 1024) and measure the **Kernel Execution Time**.
*   **Hypothesis:** Extremely small blocks (32) might underutilize the hardware, while very large blocks (1024) might limit the number of active blocks per SM due to register pressure.
*   **How:** Change `BLOCK_SIZE_VAL` in `parallel/include/common.hpp` or pass it as a compiler flag (e.g., `-DBLOCK_SIZE_VAL=128`).

## 2. Pinned vs. Pageable Memory Transfer
*   **Goal:** Quantify the benefit of using Pinned Memory over standard Pageable Memory.
*   **Experiment:** Modify `data_loader.cpp` to use a standard `std::vector<float>` or `new float[]` for the SoA data instead of `cudaMallocHost`.
*   **Measure:** Compare the **H2D Transfer Time** and **D2H Transfer Time**.
*   **Hypothesis:** Pinned memory should offer significantly higher bandwidth (approx. 2x faster transfer) compared to pageable memory, as it allows for Direct Memory Access (DMA).

## 3. Unified Memory (Managed Memory)
*   **Goal:** Evaluate the trade-off between programming ease and performance.
*   **Experiment:** Replace `cudaMalloc` and `cudaMemcpy` in `main.cpp` with `cudaMallocManaged`. Remove the explicit H2D and D2H transfer calls (prefetching is optional).
*   **Measure:** **Total Execution Time** and compare it with the explicit memory management version.
*   **Hypothesis:** Unified Memory might be slightly slower due to page fault overhead on the first access, or potentially faster if the driver optimizes data migration effectively.

## 4. Floating Point Precision (Float vs. Double)
*   **Goal:** Assess the performance cost of higher precision.
*   **Experiment:** Change `float` to `double` in your kernel, data structures, and constants.
*   **Measure:** **Kernel Execution Time** and **Memory Usage**.
*   **Hypothesis:** Consumer GPUs often have much lower FP64 (double) performance compared to FP32 (float) (often 1/32 or 1/64 rate), leading to a significant slowdown.

## 5. Loop Unrolling (Kernel Optimization)
*   **Goal:** Reduce loop overhead in the kernel.
*   **Experiment:** Add `#pragma unroll` directives to the loops inside `sad_pattern_matching_kernel` (especially the inner loop over `QUERY_LENGTH`).
*   **Measure:** **Kernel Execution Time**.

## 6. Asynchronous Execution (Streams)
*   **Goal:** Hide data transfer latency.
*   **Experiment:** Instead of transferring the entire dataset at once, split it into chunks. Use **CUDA Streams** (`cudaStream_t`) to copy chunk $i+1$ while processing chunk $i$.
*   **Note:** Since your report states that *Disk I/O* is the main bottleneck, this might not improve the *Total Time* significantly, but it improves the *GPU efficiency* (hiding PCIe latency).
